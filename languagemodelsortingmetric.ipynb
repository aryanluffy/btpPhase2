{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install language_tool_python\n!pip install bs4\n!pip install torch\n!pip install spacy\n!pip install tqdm\n!pip install transformers\n!pip install nltk","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-26T08:27:34.684065Z","iopub.execute_input":"2022-03-26T08:27:34.684618Z","iopub.status.idle":"2022-03-26T08:29:02.336016Z","shell.execute_reply.started":"2022-03-26T08:27:34.684518Z","shell.execute_reply":"2022-03-26T08:29:02.334876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf=pd.read_csv('../input/conlllabeledsentencedata/lmScoreCorrected.csv')\n# print(df)\nones=0\nfor index in df.index:\n    if df['Label'][index]:\n        ones+=1\nprint('Number of Ones: '+str(ones))","metadata":{"execution":{"iopub.status.busy":"2022-03-26T08:29:02.33898Z","iopub.execute_input":"2022-03-26T08:29:02.339379Z","iopub.status.idle":"2022-03-26T08:29:02.400526Z","shell.execute_reply.started":"2022-03-26T08:29:02.339332Z","shell.execute_reply":"2022-03-26T08:29:02.399421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import encodings\nfrom itertools import count\nimport language_tool_python\nfrom bs4 import BeautifulSoup\nfrom nltk.translate.bleu_score import sentence_bleu\nimport re\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast\nimport torch\nfrom tqdm import tqdm\nimport spacy\n\n#loading transformer model\ndevice = \"cuda\"\nmodel_id = \"gpt2-large\"\nmodel = GPT2LMHeadModel.from_pretrained(model_id).to(device)\ntokenizer = GPT2TokenizerFast.from_pretrained(model_id)\n\nimport os, psutil\nprocess = psutil.Process(os.getpid())\nprint(process.memory_info().rss)  # in bytes \n\n#loading spacy model\nnlp = spacy.load(\"en_core_web_sm\")\n\n\ntool = language_tool_python.LanguageTool('en-US')\n#instead of LanguageToolPublicAPI we can also use LanguageTool to run locally\nis_bad_rule = lambda rule: rule.message == 'Possible mistake found.' and len(rule.replacements) and rule.replacements[0][0].isupper()\n#is_bad_rule is used to identify errors.\n\ndef get_perplexity(sentenceList):\n    \"\"\"\n    This function is used to get the perplexity of the given sentence\n    :params: python List of strings\n    :return: float\n    \"\"\"\n    encodings = tokenizer(\"\\n\\n\".join(sentenceList), return_tensors=\"pt\")\n    # print(process.memory_info().rss)\n    \n\n    max_length = model.config.n_positions\n    # print(max_length)\n    stride = 1\n\n    nlls = []\n    # print(encodings.input_ids.size(1))\n    for i in tqdm(range(1, encodings.input_ids.size(1), stride)):\n        begin_loc = max(i + stride - max_length, 0)\n        end_loc = min(i + stride, encodings.input_ids.size(1))\n        trg_len = end_loc - i  # may be different from stride on last loop\n        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n        target_ids = input_ids.clone()\n        target_ids[:, :-trg_len] = -100\n        # print(target_ids)\n        with torch.no_grad():\n            outputs = model(input_ids, labels=target_ids)\n            neg_log_likelihood = outputs[0] * trg_len\n\n        nlls.append(neg_log_likelihood)\n        # print(nlls)\n    if len(nlls)==0:\n        return torch.tensor(1.0)\n    ppl = torch.exp(torch.stack(nlls).sum() / end_loc)\n    # print(ppl)\n    return ppl\n\ndef get_min_contextual_prob(sentenceList):\n    \"\"\"\n    This function is used to get the minimum contextual probability of the given sentence\n    :params: python List of strings\n    :return: float\n    \"\"\"\n    encodings = tokenizer(\"\\n\\n\".join(sentenceList), return_tensors=\"pt\")\n    # print(process.memory_info().rss)\n\n    max_length = model.config.n_positions\n    # print(max_length)\n    stride = 1\n\n    nlls = []\n    # print(encodings.input_ids.size(1))\n    for i in tqdm(range(1, encodings.input_ids.size(1), stride)):\n        begin_loc = max(i + stride - max_length, 0)\n        end_loc = min(i + stride, encodings.input_ids.size(1))\n        trg_len = end_loc - i  # may be different from stride on last loop\n        input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n        target_ids = input_ids.clone()\n        target_ids[:, :-trg_len] = -100\n        # print(target_ids)\n        with torch.no_grad():\n            outputs = model(input_ids, labels=target_ids)\n            neg_log_likelihood = outputs[0] * trg_len\n\n        nlls.append(neg_log_likelihood)\n        # print(nlls)\n    if len(nlls)==0:\n        return torch.tensor(1.0)\n    # print(nlls)\n    ppl = torch.exp(torch.stack(nlls).max())\n    # print(ppl)\n    return ppl\n\ndef get_contextual_probabilites_vector(sentence):\n    \"\"\"\n    This function is used to get the contextual probabilites vector of the given sentence\n    :params: A string \n    :return: python list of floats\n    \"\"\"\n    encodings=tokenizer(sentence,return_tensors=\"pt\")\n    max_length=model.config.n_positions\n    stride=1\n    nlls=[]\n    for i in tqdm(range(1,encodings.input_ids.size(1),stride)):\n        begin_loc=max(i+stride-max_length,0)\n        end_loc=min(i+stride,encodings.input_ids.size(1))\n        trg_len=end_loc-i\n        input_ids=encodings.input_ids[:,begin_loc:end_loc].to(device)\n        target_ids=input_ids.clone()\n        target_ids[:,:-trg_len]=-100\n        with torch.no_grad():\n            outputs=model(input_ids,labels=target_ids)\n            neg_log_likelihood=outputs[0]*trg_len\n        nlls.append(neg_log_likelihood.item())\n    if len(nlls)==0:\n        return [0.0]\n    return nlls\n\ndef get_potential_positions_for_errors(probabilityVector, scoreLimit):\n    \"\"\"\n    This function is used to get the potential positions for errors\n    :params: python List of floats\n    :return: python List of integers\n    \"\"\"\n    potential_positions = []\n    for i in range(len(probabilityVector)):\n        if probabilityVector[i] > scoreLimit:\n            potential_positions.append(i)\n    return potential_positions\n\n\ndef remove_html_tags(text):\n    \"\"\"\n    It takes a text and removes the html tags.\n    \"\"\"\n    return BeautifulSoup(text, \"html.parser\").get_text()\n\ndef bleu(data):\n    \"\"\"\n    It takes only one argument which is string and return us score of the grammetical accuracy of that string\n    :param data:\n    :return:\n    score=bleu score of given string ,\n    data= given string\n    correct_string = string without html tags\n    suggestions = suggestions suggested by bleu library\n    \"\"\"\n    matches = tool.check(data)\n    matches = [rule for rule in matches if not is_bad_rule(rule)]\n    correct_string = language_tool_python.utils.correct(data, matches)\n    print(correct_string)\n    score = sentence_bleu([correct_string.split(\" \")], data.split(\" \"), weights=(0.34, 0.33, 0.33, 0))\n    return score\n\ndef extract_sentence(last_response):\n    \"\"\"\n    This function is used to extracts text and comment from nested response\n    :param last_response:\n    :return: text and comment in dictionary format\n    \"\"\"\n    final_sentence = []\n    for response_step in last_response.get(\"steps\"):\n        for key, value in response_step.items():\n            if isinstance(value, dict):\n                for sub_key, sub_value in value.items():\n                    if sub_key == \"text\" or sub_key == \"comment\":\n                        final_sentence.append({sub_key: sub_value})\n            elif key == \"text\" or key == \"comment\":\n                final_sentence.append({key: value})\n    \n    return final_sentence\n\ndef mean_bleu_score(last_response):\n    \"\"\"\n    This function is used to extracts text and comment from nested response and calculate their bleu score\n    :param last_response:\n    :return: avg bleu score and list of score.\n    \"\"\"\n    score_list = []\n    bleu_score, no_of_sentences = 0, 0\n    for response_step in last_response.get(\"steps\"):\n        for key, value in response_step.items():\n            if isinstance(value, dict):\n                for sub_key, sub_value in value.items():\n                    if sub_key == \"text\" or sub_key == \"comment\":\n                        score_list.append(bleu(remove_html_tags(sub_value)))\n                        bleu_score = bleu_score + bleu(remove_html_tags(sub_value))\n                        no_of_sentences = no_of_sentences + 1\n            elif key == \"text\" or key == \"comment\":\n                score_list.append(bleu(remove_html_tags(value)))\n                bleu_score = bleu_score + bleu(remove_html_tags(value))\n                no_of_sentences = no_of_sentences + 1\n    \n    avg_bleu_score = bleu_score/no_of_sentences\n    print(\"avg_bleu_score = \", avg_bleu_score)\n    return score_list, avg_bleu_score\n\ndef flattenDict(nested_dict,flattenedStaticList,flattenedDynamicList):\n    \"\"\"\n    This function is used to flatten the nested dict\n    \"\"\"\n    if isinstance(nested_dict, dict):\n        if 'jinja_template' in nested_dict:\n            if isDynamicString(nested_dict['jinja_template']):\n                flattenedDynamicList.append([remove_html_tags(nested_dict['jinja_template']).replace('`',''),\n                                    remove_html_tags(nested_dict['rendered_value']).replace('`','')])\n            else:\n                flattenedStaticList.append([remove_html_tags(nested_dict['jinja_template']).replace('`',''),\n                                    remove_html_tags(nested_dict['rendered_value']).replace('`','')])\n        for key, value in nested_dict.items():\n            if key!='jinja_template':\n                flattenDict(value, flattenedStaticList,flattenedDynamicList)\n    \n    if isinstance(nested_dict, list):\n        for item in nested_dict:\n            flattenDict(item, flattenedStaticList,flattenedDynamicList)\n    \n    return\n\ndef isDynamicString(text):\n    \"\"\"\n    This function is used to check if the string is  dynamic or static \n    \"\"\"\n    dynamicSection=re.findall('{{',text)\n    newlineDynamicSection=re.findall('\\n.*{{',text)\n    if len(dynamicSection)==len(newlineDynamicSection):\n        return False\n    return True\n\n\ndef markPositionsForProperNouns(sentence):\n    \"\"\"\n    This function returns the positions of words which are proper nouns\n    :param string:\n    :return: list of positions\n    \"\"\"\n    # nlp = spacy.load(\"en_core_web_sm\")\n    doc = nlp(sentence)\n    positions=[]\n    counter=0\n    for token in doc:\n        if token.tag_ == 'NNP':\n            positions.append(counter)\n    counter+=1\n    return positions\n    \ndef normalizeSentence(sentence):\n    \"\"\"\n    This function is used to normalize the sentence\n    \"\"\"\n    properNounPositions=set(markPositionsForProperNouns(sentence))\n    normalizedSentence=''\n    counter=0\n    for word in sentence:\n        if counter in properNounPositions:\n            normalizedSentence+='Alice '\n        else:\n            normalizedSentence+=word\n        normalizedSentence+=' '\n        counter+=1\n    normalizedSentence=normalizedSentence[0:-1]\n    return normalizedSentence","metadata":{"execution":{"iopub.status.busy":"2022-03-26T08:29:02.402548Z","iopub.execute_input":"2022-03-26T08:29:02.403138Z","iopub.status.idle":"2022-03-26T08:31:56.851844Z","shell.execute_reply.started":"2022-03-26T08:29:02.403092Z","shell.execute_reply":"2022-03-26T08:31:56.84974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data=[]\nheaders=['Sentence','Label','sum','min','max/min','max/med','med/min','med']","metadata":{"execution":{"iopub.status.busy":"2022-03-26T08:31:56.856026Z","iopub.execute_input":"2022-03-26T08:31:56.857441Z","iopub.status.idle":"2022-03-26T08:31:56.870564Z","shell.execute_reply.started":"2022-03-26T08:31:56.857392Z","shell.execute_reply":"2022-03-26T08:31:56.869062Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for index in df.index:\n    row=[]\n    probVector=get_contextual_probabilites_vector(df['Sentence'][index])\n    probVector=sorted(probVector,reverse=True)\n    perplexity=0.0\n    for x in probVector:\n        perplexity+=x\n    maxLogLiklihood=probVector[0]\n    minLogLiklihood=probVector[-1]\n    medianLogLiklihood=probVector[len(probVector)//2]\n    row.append(df['Sentence'][index])\n    row.append(df['Label'][index])\n    row.append(perplexity)\n    row.append(maxLogLiklihood)\n    row.append(maxLogLiklihood-minLogLiklihood)\n    row.append(maxLogLiklihood-medianLogLiklihood)\n    row.append(medianLogLiklihood-minLogLiklihood)\n    row.append(medianLogLiklihood)\n    data.append(row)","metadata":{"execution":{"iopub.status.busy":"2022-03-26T08:31:56.876976Z","iopub.execute_input":"2022-03-26T08:31:56.877725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newDf=pd.DataFrame(data,columns=headers)\nnewDf.to_csv('additionalLmScores.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}